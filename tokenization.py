# from tensorflow.keras.preprocessing.text import Tokenizer

# sentences = [
#     'i love my dog',
#     'I, love my cat',
#     'You love my dog!',
#     'hello hello hello hello'
# ]

# tokenizer = Tokenizer(num_words = 100)
# tokenizer.fit_on_texts(sentences)
# word_index = tokenizer.word_index
# print(word_index)